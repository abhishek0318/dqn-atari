from collections import deque
from datetime import datetime
import random
import time

import gym
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
from skimage import color
from skimage.transform import resize

INFINITY = 10 ** 20

class DeepQNet:
    def __init__(self, env_name):
        self.env_name = env_name
        self.action_space = gym.make(self.env_name).action_space
        self.net = self.Net(num_outputs=self.action_space.n)

    class Net(nn.Module):
        # Code taken from https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html

        def __init__(self, num_outputs):
            super().__init__()

            self.conv1 = nn.Conv2d(4, 32, 8, 4)
            self.conv2 = nn.Conv2d(32, 64, 4, 2)
            self.conv3 = nn.Conv2d(64, 64, 3, 1)
            self.fc1 = nn.Linear(64 * 7 * 7, 512)
            self.fc2 = nn.Linear(512, num_outputs)

        def forward(self, x):
            # x: (batch_size, 4, 84, 84)
            x = F.relu(self.conv1(x))
            x = F.relu(self.conv2(x))
            x = F.relu(self.conv3(x))
            x = x.view(-1, self.num_flat_features(x))
            x = F.relu(self.fc1(x))
            x = self.fc2(x)
            return x

        def num_flat_features(self, x):
            size = x.size()[1:]  # all dimensions except the batch dimension
            num_features = 1
            for dim in size:
                num_features *= dim
            return num_features

    class ReplayMemory:
        # Code taken from https://stackoverflow.com/questions/40181284/how-to-get-random-sample-from-deque-in-python-3

        def __init__(self, max_size):
            max_size = int(max_size)
            self.buffer = [None] * max_size
            self.max_size = max_size
            self.index = 0
            self.size = 0

        def append(self, obj):
            self.buffer[self.index] = obj
            self.size = min(self.size + 1, self.max_size)
            self.index = (self.index + 1) % self.max_size

        def sample(self, batch_size):
            sample_size = min(batch_size, self.size)
            indices = random.sample(range(self.size), sample_size)
            return [self.buffer[index] for index in indices]

    def _clip(self, reward):
        if reward > 0:
            return min(reward, 1)
        elif reward == 0:
            return 0
        else:
            return max(reward, -1)

    def random_action(self):
        return self.action_space.sample()

    def best_action(self, state):
        device = next(self.net.parameters()).device
        state = state.to(device)
        return self.net(state.unsqueeze(0)).squeeze(0).argmax().item()

    def process_image(self, image):
        #TODO: handle flickering
        image = color.rgb2gray(image) # convert to grayscale
        image = resize(image, (110, 84), anti_aliasing=False) # downsample
        image = image[18:102, :] # crop image
        return image

    def make_state(self, state):
        state = np.stack(state, axis=0)
        # if images experiences by agent is less than 4, pad remaining state with 0
        if state.shape[0] < 4:
            padding = np.zeros((4 - state.shape[0], 84, 84))
            state = np.concatenate([state, padding], axis=0)
        return torch.FloatTensor(state)

    def update_net(self, batch, target_net, optimizer, discount_factor):
        # batch: [(state, action, reward, new_state, finished)] * batch_size

        device = next(self.net.parameters()).device
        state = torch.stack([x[0] for x in batch], dim=0).to(device)
        action = torch.LongTensor([x[1] for x in batch]).to(device)
        reward = torch.FloatTensor([x[2] for x in batch]).to(device)
        new_state = torch.stack([x[3] for x in batch], dim=0).to(device)
        finished = torch.FloatTensor([x[4] for x in batch]).to(device)

        max_q = target_net(new_state).max(dim=1)[0]
        mask = 1 - finished
        max_q *= mask
        target = reward + discount_factor * max_q
        target.detach_()

        q = self.net(state)[range(target.shape[0]), action]

        loss = (target - q) ** 2
        loss.clamp_(min=-1, max=1)
        loss = loss.sum()

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        return loss.item()

    def initialise_replay_memory(self, replay_memory, replay_start_size=50000):
        pbar = tqdm(total=replay_start_size)

        env = gym.make(self.env_name)
        for episode in range(INFINITY):
            image = env.reset()
            images = deque([self.process_image(image)], maxlen=4)

            finished = False
            while not finished:
                pbar.update()

                state = self.make_state(images)
                action = self.random_action()
                image, reward, finished, _ = env.step(action)

                images.append(self.process_image(image))
                new_state = self.make_state(images)
                replay_memory.append((state, action, self._clip(reward), new_state, finished))

                if replay_memory.size == replay_start_size:
                    pbar.close()
                    env.close()
                    return

    def linear_decay(self, frame_number, final_exploration_frame=1000000, initial_exploration=1,
                     final_exploration=0.1):

        if frame_number > final_exploration_frame:
            return final_exploration
        else:
            difference = initial_exploration - final_exploration
            return initial_exploration - difference * ((frame_number - 1) / final_exploration_frame)

    def train(self, training_frames=10000000, minibatch_size=32, replay_memory_size=1000000,
              target_network_update_frequency=10000, discount_factor=0.99, learning_rate=0.00025,
              initial_exploration=1, final_exploration=0.1, final_exploration_frame=1000000,
              replay_start_size=50000, checkpoint_path=None):

        # initialise replay memory
        replay_memory = self.ReplayMemory(replay_memory_size)
        self.initialise_replay_memory(replay_memory, replay_start_size)

        # intialise action value function q with random weights
        self.net.train()
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.net.to(device)

        # initialise target action value function
        target_net = self.Net(self.action_space.n)
        target_net.to(device)
        target_net.load_state_dict(self.net.state_dict())

        # initialise environmnent and optimiser
        env = gym.make(self.env_name)
        optimizer = optim.RMSprop(self.net.parameters(), lr=learning_rate)
        epsilon_fn = lambda x: self.linear_decay(x, final_exploration_frame,
                                                 initial_exploration, final_exploration)

        # initialise logging related things
        writer = SummaryWriter()
        pbar = tqdm(total=training_frames)

        # initialise counts
        frame_number = 0
        net_updates = 0

        for episode in range(INFINITY):
            # initialise statistics for logging
            episode_reward = 0
            episode_loss = 0
            episode_net_updates = 0

            # initialise sequence
            image = env.reset()
            images = deque([self.process_image(image)], maxlen=4)

            finished = False
            while not finished:
                frame_number += 4
                net_updates += 1
                pbar.update(4)

                state = self.make_state(images)
                # select random action with epsilon probability else select best action
                epsilon = epsilon_fn(frame_number)
                action = self.best_action(state) if random.random() > epsilon else self.random_action()
                # execute action in emulator and obtain next image, reward
                image, reward, finished, _ = env.step(action)

                images.append(self.process_image(image))
                new_state = self.make_state(images)
                # store transition in replay_memory
                replay_memory.append((state, action, self._clip(reward), new_state, finished))

                # Sample batch from replay memory and update parameters
                batch = replay_memory.sample(minibatch_size)
                loss = self.update_net(batch, target_net, optimizer, discount_factor)

                # update the target net after target_network_update_frequency steps
                if net_updates % target_network_update_frequency == 0:
                    target_net.load_state_dict(self.net.state_dict())

                # save the network if desired
                if checkpoint_path and frame_number % (training_frames // 10) == 0:
                    current_time = datetime.today().strftime('%Y-%m-%d-%H:%M:%S')
                    path = "{}_{}.pt".format(checkpoint_path, current_time)
                    self.save(path)
                    self.net.to(device)

                # maintain statistics for logging
                episode_net_updates += 1
                episode_reward += reward
                episode_loss += loss

                # stop training
                if frame_number == training_frames:
                    pbar.close()
                    env.close()
                    writer.close()
                    return

            writer.add_scalar("episode_reward", episode_reward, frame_number)
            writer.add_scalar("episode_loss", episode_loss / episode_net_updates, frame_number)

    def play(self, num_frames=2000):
        self.net.eval()
        self.net.to("cpu")
        env = gym.make(self.env_name)

        frame_number = 0
        for episode in range(INFINITY):
            image = env.reset()
            images = deque([self.process_image(image)], maxlen=4)

            finished = False
            while not finished:
                frame_number += 4

                env.render()
                time.sleep(0.03)

                state = self.make_state(images)
                action = self.best_action(state) if random.random() > 0.05 else self.random_action()
                image, reward, finished, _ = env.step(action)
                images.append(self.process_image(image))

                if frame_number > num_frames:
                    env.close()
                    return

    def save(self, path):
        self.net.to("cpu")
        torch.save(self.net.state_dict(), path)

    def load(self, path):
        self.net.to("cpu")
        self.net.load_state_dict(torch.load(path))
